# MAMBA Diffusion V3 - Morton Curve Spatial Ordering

Clean architectural improvement using space-filling curves for better spatial locality in MAMBA processing.

---

## 🎯 Problem Solved

**V1 Issue**: Row-major flattening breaks spatial locality
- Pixel `(0, 31)` and `(1, 0)` are FAR apart spatially
- But they're ADJACENT in the sequence
- MAMBA's sequential processing doesn't understand 2D spatial structure

**Result**: Poor spatial coherence in state propagation

---

## ✨ V3 Solution: Morton Curves (Z-Order)

### What are Morton Curves?

Morton curves (Z-order curves) interleave the bits of x and y coordinates to create a 1D ordering that preserves 2D spatial locality.

**Example 4×4 grid:**
```
Row-major order:          Morton (Z-order):
0  1  2  3              0  1  4  5
4  5  6  7              2  3  6  7
8  9  10 11             8  9  12 13
12 13 14 15             10 11 14 15
```

**Key benefit**: Neighboring pixels in 2D are also neighbors in the 1D sequence!

### How V3 Works

```python
# V1: Row-major ordering (bad locality)
sequence = flatten_row_major(pixels)  # (0,0), (0,1), ..., (0,31), (1,0), ...
mamba_output = mamba(sequence)

# V3: Morton ordering (good locality)
sequence = flatten_morton_curve(pixels)  # Spatially coherent order
mamba_output = mamba(sequence)
sequence = restore_original_order(mamba_output)
```

**Process**:
1. Reorder query tokens using Morton curve
2. Process through MAMBA (same architecture as V1)
3. Restore original order for output

---

## 📊 Architecture Comparison

| Aspect | V1 (Baseline) | V3 (Morton) |
|--------|--------------|-------------|
| **MAMBA layers** | 6 unidirectional | 6 unidirectional |
| **d_model** | 512 | 512 |
| **Parameters** | ~15M | ~15M |
| **Sequence ordering** | Row-major | Morton curve |
| **Spatial locality** | Poor | Good |
| **Computational cost** | 1.0x | 1.0x (identical) |

**Key difference**: Only the sequence ordering changes, everything else is identical to V1!

---

## 🚀 Usage

### Training V3

```bash
cd v3/training
./run_mamba_v3_training.sh
```

### Custom Configuration

```bash
# Same parameters as V1
D_MODEL=512 NUM_LAYERS=6 BATCH_SIZE=64 ./run_mamba_v3_training.sh
```

### Monitor Training

```bash
tail -f training_v3_output.log
```

---

## 📈 Expected Improvements

### V1 vs V3

| Metric | V1 | V3 (Expected) | Improvement |
|--------|----|--------------| ------------|
| **Spatial coherence** | Poor | Good | Better |
| **Background artifacts** | Present | Reduced | -20-30% |
| **PSNR** | ~28 dB | ~29-30 dB | +1-2 dB |
| **Training time** | 1.0x | 1.0x | Same |

**Why improvements are modest**:
- V3 is a clean, focused improvement
- Better ordering helps, but architecture is still V1
- For larger gains, combine with V2's bidirectional processing

---

## 🔬 Technical Details

### Morton Encoding Algorithm

```python
def morton_encode(x, y):
    """Interleave bits of x and y"""
    # x = 3 = 0b11, y = 2 = 0b10
    # Morton code = 0b1110 = 14
    return part1by1(x) | (part1by1(y) << 1)

# For 32×32 grid:
# (0, 0) → 0
# (1, 0) → 1
# (0, 1) → 2
# (1, 1) → 3
# Better locality than row-major!
```

### Implementation in Model

```python
class MAMBADiffusion(nn.Module):
    def __init__(self, ...):
        # Precompute Morton indices
        morton_indices = get_morton_order_indices(resolution)
        self.register_buffer('morton_indices', morton_indices)
        self.register_buffer('inverse_morton_indices',
                           torch.argsort(morton_indices))

    def forward(self, noisy_values, query_coords, ...):
        # Encode features
        query_tokens = self.query_proj(...)

        # Reorder using Morton curve
        query_tokens = query_tokens[:, self.morton_indices, :]

        # Process with MAMBA
        seq = torch.cat([input_tokens, query_tokens], dim=1)
        for mamba in self.mamba_blocks:
            seq = mamba(seq)

        # Restore original order
        query_seq = seq[:, N_in:, :]
        query_seq = query_seq[:, self.inverse_morton_indices, :]

        return self.decoder(query_seq)
```

---

## 📁 File Structure

```
v3/
└── training/
    ├── train_mamba_v3_morton.py        # V3 implementation
    └── run_mamba_v3_training.sh        # Training runner

core/
└── spatial_ordering.py                 # Morton curve utilities
```

---

## 🎨 Visualization

### Sequence Ordering Comparison

**Row-major (V1)**:
- Processes rows left-to-right, top-to-bottom
- Jumps from end of row to start of next row
- Poor spatial locality at row boundaries

**Morton curve (V3)**:
- Follows Z-shaped pattern recursively
- Stays in local spatial neighborhoods
- Better locality throughout sequence

![Morton Ordering](../morton_ordering.png)
*(Generated by `core/spatial_ordering.py`)*

---

## 🔧 Configuration Options

### Enable/Disable Morton Ordering

```python
model = MAMBADiffusion(
    d_model=512,
    num_layers=6,
    use_morton_ordering=True,   # Enable Morton curves
    image_resolution=32         # Grid size for ordering
)
```

### Different Resolutions

V3 automatically handles different resolutions:
- 32×32: Full Morton ordering
- 64×64, 96×96, etc.: Morton ordering applied
- Arbitrary sizes: Falls back to row-major if not square grid

---

## 🆚 Comparison with Other Versions

### V1 vs V2 vs V3

| Feature | V1 (Baseline) | V2 (Complex) | V3 (Clean) |
|---------|--------------|--------------|-----------|
| **Ordering** | Row-major | Row-major | Morton curve |
| **MAMBA** | 6 unidirectional | 8 bidirectional | 6 unidirectional |
| **Attention** | 1 cross-attn | Perceiver + self-attn | 1 cross-attn |
| **Parameters** | 15M | 7M | 15M |
| **Complexity** | 1.0x | 1.7x | 1.0x |
| **Focus** | Baseline | Architectural | Ordering |

**V3 Philosophy**: Simple, clean improvement with identical computational cost

---

## 📊 Training Configuration

### Default Parameters

```bash
d_model=512          # Same as V1
num_layers=6         # Same as V1
batch_size=64
lr=1e-4
epochs=1000
morton_ordering=True # NEW: Enabled by default
resolution=32        # Grid size
```

### Checkpoints

```
checkpoints_mamba_v3/
├── mamba_v3_best.pth      # Best validation loss
├── mamba_v3_latest.pth    # Latest epoch
└── mamba_v3_epoch_XXXX.pth # Periodic saves
```

---

## 🧪 Evaluation

### Compare V1 vs V3

```bash
cd v3/evaluation
python eval_v1_vs_v3.py \
    --v1_checkpoint ../../v1/training/checkpoints_mamba/mamba_best.pth \
    --v3_checkpoint ../training/checkpoints_mamba_v3/mamba_v3_best.pth
```

---

## 🎯 When to Use V3

**Use V3 if**:
- You want better spatial coherence without added complexity
- Computational budget is tight (same cost as V1)
- You value clean, understandable improvements
- You want to combine with other techniques later

**Use V2 if**:
- You need maximum quality regardless of cost
- You can afford ~70% more computation
- Speckle artifacts are a major issue

**Combine V2+V3?**:
- Could apply Morton ordering to V2's bidirectional MAMBA
- Best of both worlds: better ordering + better architecture
- TODO: Implement V4 = V2 + Morton curves

---

## 📚 References

- **Morton Curves**: Morton, G.M. (1966) - "A Computer Oriented Geodetic Data Base"
- **Space-Filling Curves**: Sagan, H. (1994) - "Space-Filling Curves"
- **Z-Order in Graphics**: Used extensively in quadtrees, spatial indexing
- **MAMBA**: Gu & Dao (2023) - Mamba: Linear-Time Sequence Modeling

---

## ✅ Success Indicators

After training V3, you should see:
- [ ] Better spatial smoothness in generated images
- [ ] Fewer random speckles in backgrounds
- [ ] +1-2 dB PSNR improvement over V1
- [ ] Similar training time to V1

---

## 🎉 Summary

**V3 = V1 + Morton Curves**

- ✅ Clean, simple improvement
- ✅ Better spatial locality for MAMBA
- ✅ Same computational cost as V1
- ✅ Easy to understand and implement
- ✅ Can be combined with other improvements (V2, etc.)

**Start training:**
```bash
cd v3/training
./run_mamba_v3_training.sh
```

---

**Made with ❤️ for spatially-aware sequence processing**
